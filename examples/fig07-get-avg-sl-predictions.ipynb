{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac0ccba2-463c-4164-98aa-f63d75f2de75",
   "metadata": {},
   "source": [
    "# Find the average predictions from a particular ML branch\n",
    "This notebook is a helper that grabs data and processes it into an intermediate result for storage and plotting later. The issue is that the SuperLearner workflow filters out some points during the PCA stage (to find out how different points are from other points for ICON-ModEx iterations) so the final files in `./output_data` do not contain as many points as the initial prediction set. We don't want to filter out those points - we want to use all predictions that are made at the WHONDRS sites. This notebook grabs data from `./ml_models/sl_?/sl_predictions.csv` for each SuperLearner ensemble member, averages those values together, and then saves the file for plotting later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02b27b1f-bad1-4752-987f-400eb3e6264f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b3ac30-670c-47c0-a3d8-ceedafe7ec4d",
   "metadata": {},
   "source": [
    "## Grab data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dc832bb-1bfa-43de-8c5d-55c72c853f4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'sl-archive-whondrs' already exists and is not an empty directory.\n",
      "Checking out files: 100% (253/253), done.\n",
      "Switched to branch 'S19S-SSS-log10-extrap-r01'\n"
     ]
    }
   ],
   "source": [
    "repo_prefix = '~/tmp/'\n",
    "repo_name = 'sl-archive-whondrs'\n",
    "repo_url = 'https://github.com/parallelworks/'+repo_name\n",
    "branch = 'S19S-SSS-log10-extrap-r01'\n",
    "\n",
    "# Grab the data and get onto the branch if not already there\n",
    "! mkdir -p {repo_prefix}\n",
    "! cd {repo_prefix}; git clone {repo_url}\n",
    "! cd {repo_prefix}/{repo_name}; git checkout {branch}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8fe2d0-5482-48ee-bda3-fec0a3f2e4ec",
   "metadata": {},
   "source": [
    "## Load data from each SuperLearner ensemble member and then take the average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e7a288e-f5bd-4b39-b4ef-a08c9aeadfa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_sl = 10\n",
    "\n",
    "ml_output_df_list = []\n",
    "for ll in range(0,num_sl):\n",
    "    ml_output_df_list.append(pd.read_csv(repo_prefix+repo_name+'/ml_models/sl_'+str(int(ll))+'/sl_predictions.csv'))\n",
    "\n",
    "ml_output_all_df = pd.concat(ml_output_df_list)\n",
    "by_id = ml_output_all_df.groupby(ml_output_all_df['Sample_ID'])\n",
    "predict_avg = by_id.mean()\n",
    "predict_std = by_id.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fe8adb7-bb56-4778-b1b8-ae2a3a0bed8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predict_avg.to_csv('WHONDRS_'+branch+'_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8430c64a-634b-4ab9-a255-6766e41f6392",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
